---
phase: 06-seo-and-deployment
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - app/layout.tsx
  - app/page.tsx
  - app/about/page.tsx
  - app/resume/page.tsx
  - app/projects/page.tsx
  - app/projects/[slug]/page.tsx
  - app/stack/page.tsx
  - app/contact/page.tsx
  - app/sitemap.ts
  - app/robots.ts
autonomous: true

must_haves:
  truths:
    - "Every public page has unique title in browser tab"
    - "Every public page has meta description visible in search results"
    - "Social media shares show OpenGraph image, title, and description"
    - "Search engines can discover all published project pages via sitemap"
    - "Admin routes are blocked from search engine indexing"
  artifacts:
    - path: "app/layout.tsx"
      provides: "Root metadata template"
      contains: "export const metadata"
      min_lines: 5
    - path: "app/sitemap.ts"
      provides: "Dynamic sitemap from Convex data"
      exports: ["default"]
      min_lines: 30
    - path: "app/robots.ts"
      provides: "Robots.txt with sitemap reference"
      exports: ["default"]
      min_lines: 10
    - path: "app/projects/[slug]/page.tsx"
      provides: "Dynamic metadata generation"
      exports: ["generateMetadata"]
  key_links:
    - from: "app/sitemap.ts"
      to: "convex/projects.ts"
      via: "fetchQuery(api.projects.listPublished)"
      pattern: "fetchQuery.*api\\.projects\\.listPublished"
    - from: "app/projects/[slug]/page.tsx"
      to: "convex/projects.ts"
      via: "generateMetadata fetches project data"
      pattern: "generateMetadata.*fetchQuery"
---

<objective>
Implement comprehensive SEO metadata across all public pages using Next.js 16 built-in Metadata API, generate dynamic sitemap from Convex data, and configure robots.txt to control search engine crawling.

Purpose: Make site discoverable in search engines, enable proper social media sharing with OpenGraph tags, and provide search engines with sitemap of all published projects.

Output: All pages have unique titles and descriptions, sitemap.xml dynamically generated, robots.txt blocks admin routes, canonical URLs set.
</objective>

<execution_context>
@D:\Projects\saad-portfolio\.claude\get-shit-done\workflows\execute-plan.md
@D:\Projects\saad-portfolio\.claude\get-shit-done\templates\summary.md
</execution_context>

<context>
@D:\Projects\saad-portfolio\.planning\PROJECT.md
@D:\Projects\saad-portfolio\.planning\ROADMAP.md
@D:\Projects\saad-portfolio\.planning\STATE.md
@D:\Projects\saad-portfolio\.planning\phases\06-seo-and-deployment\06-RESEARCH.md

# Existing page structure
@D:\Projects\saad-portfolio\app\layout.tsx
@D:\Projects\saad-portfolio\app\page.tsx
@D:\Projects\saad-portfolio\app\about\page.tsx
@D:\Projects\saad-portfolio\app\resume\page.tsx
@D:\Projects\saad-portfolio\app\projects\page.tsx
@D:\Projects\saad-portfolio\app\projects\[slug]\page.tsx
@D:\Projects\saad-portfolio\app\stack\page.tsx
@D:\Projects\saad-portfolio\app\contact\page.tsx
</context>

<tasks>

<task type="auto">
  <name>Add metadata to root layout and static pages</name>
  <files>
    app/layout.tsx
    app/page.tsx
    app/about/page.tsx
    app/resume/page.tsx
    app/projects/page.tsx
    app/stack/page.tsx
    app/contact/page.tsx
  </files>
  <action>
1. In app/layout.tsx, add root metadata object with title template, default description, OpenGraph base config, and robots config:
   - Title template: '%s | Portfolio' (creates "About | Portfolio" pattern)
   - Default title: 'Portfolio'
   - Description: Brief positioning statement about automated workflows
   - OpenGraph: type: 'website', locale: 'en_US', siteName: 'Portfolio'
   - Twitter card: 'summary_large_image'
   - Robots: index: true, follow: true

2. For each static page (home, about, resume, projects, stack, contact), export const metadata object with:
   - title: Descriptive page title (e.g., "About", "Resume", "Projects")
   - description: 1-2 sentence summary of page content
   - openGraph: title, description, type: 'website', url (absolute)
   - alternates.canonical: Absolute URL (e.g., 'https://yourdomain.com/about')

   Use placeholder domain 'https://yourdomain.com' - will be replaced with actual domain during deployment.

3. For home page (app/page.tsx), set title: null to use layout's default title ('Portfolio')

IMPORTANT: Use `export const metadata: Metadata = {...}` pattern for static pages. Do NOT use generateMetadata function for non-dynamic routes. Import type Metadata from 'next'.
  </action>
  <verify>
npm run build succeeds without metadata errors
Check build output for metadata generation logs
  </verify>
  <done>
All static pages have metadata exports with title, description, openGraph, and canonical URL
Root layout has title template and shared OpenGraph config
  </done>
</task>

<task type="auto">
  <name>Add dynamic metadata to project detail pages</name>
  <files>app/projects/[slug]/page.tsx</files>
  <action>
1. In app/projects/[slug]/page.tsx, add async generateMetadata function:
   - Accept params (already unwrapped with await in existing code)
   - Use fetchQuery to get project by slug from api.projects.getBySlug
   - If project not found, return { title: 'Project Not Found', robots: { index: false } }
   - If project found, return metadata with:
     - title: `${project.title} | Projects`
     - description: project.summary
     - openGraph: title, description, type: 'article', url: `https://yourdomain.com/projects/${slug}`
     - alternates.canonical: `https://yourdomain.com/projects/${slug}`

2. Import type Metadata from 'next'

IMPORTANT: The existing page component already fetches the same project data with fetchQuery. Next.js 16 automatically memoizes identical fetch requests, so there's NO duplicate fetch - generateMetadata and page component share the same data fetch.
  </action>
  <verify>
npm run build succeeds
Check that generateStaticParams builds static pages for all projects
Inspect .next/server/app/projects/[slug] for generated metadata
  </verify>
  <done>
Project detail pages have generateMetadata function that fetches project data and returns dynamic title, description, and OpenGraph tags
Next.js build confirms static generation with metadata for each project slug
  </done>
</task>

<task type="auto">
  <name>Create dynamic sitemap and robots.txt</name>
  <files>
    app/sitemap.ts
    app/robots.ts
  </files>
  <action>
1. Create app/sitemap.ts:
   - Export async default function returning Promise<MetadataRoute.Sitemap>
   - Import MetadataRoute from 'next'
   - Import { fetchQuery } from 'convex/nextjs' and api from '@/convex/_generated/api'
   - Set baseUrl = 'https://yourdomain.com'
   - Define static pages array with entries for:
     - / (priority: 1, changeFrequency: 'monthly')
     - /about (priority: 0.8, changeFrequency: 'monthly')
     - /resume (priority: 0.8, changeFrequency: 'monthly')
     - /projects (priority: 0.9, changeFrequency: 'weekly')
     - /stack (priority: 0.7, changeFrequency: 'monthly')
     - /contact (priority: 0.5, changeFrequency: 'yearly')
   - Fetch all published projects with: await fetchQuery(api.projects.listPublished)
   - Map projects to sitemap entries with url, lastModified: new Date(project._creationTime), changeFrequency: 'monthly', priority: 0.7
   - Return [...staticPages, ...projectPages]

2. Create app/robots.ts:
   - Export default function returning MetadataRoute.Robots
   - Import MetadataRoute from 'next'
   - Return object with:
     - rules: { userAgent: '*', allow: '/', disallow: ['/admin/', '/auth/', '/api/'] }
     - sitemap: 'https://yourdomain.com/sitemap.xml'

IMPORTANT: Sitemap fetches data at build time (or request time if dynamic). This is intentional - search engines recrawl sitemaps periodically, so perfect freshness is not required.
  </action>
  <verify>
npm run build succeeds
Visit http://localhost:3000/sitemap.xml after dev server starts - should see XML with all static pages and published projects
Visit http://localhost:3000/robots.txt - should see disallow rules and sitemap reference
  </verify>
  <done>
sitemap.ts generates dynamic sitemap including all static pages and published project pages
robots.txt blocks /admin/, /auth/, /api/ from indexing and references sitemap
Both files accessible at their standard URLs (/sitemap.xml, /robots.txt)
  </done>
</task>

</tasks>

<verification>
## Build Verification
- npm run build completes without metadata warnings
- Build output shows sitemap.xml and robots.txt generation

## Runtime Verification
- Visit http://localhost:3000 and check browser tab title shows "Portfolio"
- Visit http://localhost:3000/about and check tab shows "About | Portfolio"
- View page source and verify meta tags:
  - <title> tag present
  - <meta name="description"> present
  - <meta property="og:title"> present
  - <meta property="og:description"> present
  - <link rel="canonical"> present
- Visit http://localhost:3000/sitemap.xml - should render XML with all pages
- Visit http://localhost:3000/robots.txt - should show disallow rules
- Check that no console errors about metadata configuration
</verification>

<success_criteria>
- All 6 SEO requirements covered: SEO-01 (unique titles), SEO-02 (meta descriptions), SEO-03 (OpenGraph tags), SEO-04 (sitemap.xml), SEO-05 (robots.txt), SEO-06 (canonical URLs)
- Every static page (home, about, resume, projects, stack, contact) has metadata export
- Project detail pages use generateMetadata for dynamic titles
- Sitemap includes all static pages and dynamically fetches published projects
- Robots.txt blocks admin routes and references sitemap
- Build succeeds with no metadata errors
- All metadata URLs are absolute with placeholder domain ready for production replacement
</success_criteria>

<output>
After completion, create `D:\Projects\saad-portfolio\.planning\phases\06-seo-and-deployment\06-01-SUMMARY.md`
</output>
